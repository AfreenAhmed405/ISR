{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: huggingface-hub 0.16.4\n",
      "Uninstalling huggingface-hub-0.16.4:\n",
      "  Successfully uninstalled huggingface-hub-0.16.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping sentence-transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y huggingface_hub sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub==0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (0.16.4)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub==0.16.4) (3.18.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub==0.16.4) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub==0.16.4) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub==0.16.4) (4.66.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub==0.16.4) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub==0.16.4) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub==0.16.4) (23.2)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers==2.2.2)\n",
      "  Using cached transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (0.21.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "  Using cached transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "  Using cached transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub==0.16.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub==0.16.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub==0.16.4) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub==0.16.4) (2024.8.30)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->sentence-transformers==2.2.2) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\n",
      "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.8.2 requires tokenizers>=0.19.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "vllm 0.8.2 requires transformers>=4.48.2, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub==0.16.4 sentence-transformers==2.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import cached_download\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB Successfully Created!\n"
     ]
    }
   ],
   "source": [
    "# Creating the vector store\n",
    "loader = DirectoryLoader('/Users/sudheerb/Documents/Academics /2nd semester/ISR/Project/Medical-Information-retrieval-with-RAG/pdf', glob=\"**/*.pdf\", show_progress=True, loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "vector_store = Chroma.from_documents(texts, embeddings, collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory= \"stores/pet_cosine\")\n",
    "\n",
    "print(\"Vector DB Successfully Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sudheerb/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c4c351f2-8078-47de-8b8d-c82732f18933)')' thrown while requesting HEAD https://huggingface.co/epfl-llm/meditron-7b/resolve/main/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830b9a546e614cb0bcc5363824dfb6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ad0f8941cb43f3aefc00224f60a600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184fc1ca57524a5c8c39df79d7b515c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a49212d97c4f0aaa380899bce10f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a56133692e243c583271c9e6b99d4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2146c02e5c4632a7a1118395d47f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72498f525ac049f69458a3427bf7ae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/262M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403bec28e72e42179ea3f4bf9f4cf487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4fe82f027540b192d5a325509a078d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Your Hugging Face access token (you can store it in an env variable instead)\n",
    "token = \"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\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"epfl-llm/meditron-7b\", token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"epfl-llm/meditron-7b\",\n",
    "    token=token,\n",
    "    resume_download=True,            # try to resume partial downloads\n",
    "    local_files_only=False,          # ensure it uses internet\n",
    "    trust_remote_code=True,          # if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Wrap it as a LangChain-compatible LLM\n",
    "llm = HuggingFacePipeline(pipeline=text_gen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a diagnostic tool assistant. Don't show any hate,\n",
    "abusive, racist, type of behavior. Be as kind and professional as possible.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NeuML/pubmedbert-base-embeddings\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "\n",
    "load_vector_store = Chroma(persist_directory=\"stores/pet_cosine\", embedding_function=embeddings)\n",
    "\n",
    "retriever = load_vector_store.as_retriever(search_kwargs={\"k\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"How many are the waves of the QRS complex that represent ventricular depolarization. Keep the answer short\"  \n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "response = qa(query)\n",
    "answer = response['result']\n",
    "source_document = response['source_documents'][0].page_content\n",
    "doc = response['source_documents'][0].metadata['source']\n",
    "response_data = {\"answer\": answer, \"source_document\": source_document, \"doc\": doc, \"page_number\": page_number}\n",
    "\n",
    "# If you want to print or use the response_data\n",
    "response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many are the waves of the QRS complex that represent ventricular depolarization. Keep the answer short\"  \n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "response = qa(query)\n",
    "answer = response['result']\n",
    "source_document = response['source_documents'][0].page_content\n",
    "doc = response['source_documents'][0].metadata['source']\n",
    "page_number = response['source_documents'][0].metadata.get('page_number', 'Not available')\n",
    "response_data = {\"answer\": answer, \"source_document\": source_document, \"doc\": doc, \"page_number\": page_number}\n",
    "\n",
    "# If you want to print or use the response_data\n",
    "response_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
